{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73588c0b",
   "metadata": {},
   "source": [
    "Kaggle Competition: BBC News Classification\n",
    "A Jupyter notebook with exploratory data analysis (EDA) procedure, model building and training, and comparison with supervised learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d174a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2709a4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"https://raw.githubusercontent.com/gt2onew/dtsa5510/main/week4/BBC%20News%20Train.csv\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76751c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"https://raw.githubusercontent.com/gt2onew/dtsa5510/main/week4/BBC%20News%20Test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f83d8127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1833</td>\n",
       "      <td>worldcom ex-boss launches defence lawyers defe...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154</td>\n",
       "      <td>german business confidence slides german busin...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1101</td>\n",
       "      <td>bbc poll indicates economic gloom citizens in ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1976</td>\n",
       "      <td>lifestyle  governs mobile choice  faster  bett...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>917</td>\n",
       "      <td>enron bosses in $168m payout eighteen former e...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ArticleId                                               Text  Category\n",
       "0       1833  worldcom ex-boss launches defence lawyers defe...  business\n",
       "1        154  german business confidence slides german busin...  business\n",
       "2       1101  bbc poll indicates economic gloom citizens in ...  business\n",
       "3       1976  lifestyle  governs mobile choice  faster  bett...      tech\n",
       "4        917  enron bosses in $168m payout eighteen former e...  business"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fabbd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1490 entries, 0 to 1489\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   ArticleId  1490 non-null   int64 \n",
      " 1   Text       1490 non-null   object\n",
      " 2   Category   1490 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 35.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c030dc13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category\n",
       "sport            346\n",
       "business         336\n",
       "politics         274\n",
       "entertainment    273\n",
       "tech             261\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.Category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "678a1f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1490"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.Category.value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0458b3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 735 entries, 0 to 734\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   ArticleId  735 non-null    int64 \n",
      " 1   Text       735 non-null    object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 11.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abb6c069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1018</td>\n",
       "      <td>qpr keeper day heads for preston queens park r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1319</td>\n",
       "      <td>software watching while you work software that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1138</td>\n",
       "      <td>d arcy injury adds to ireland woe gordon d arc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>459</td>\n",
       "      <td>india s reliance family feud heats up the ongo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1020</td>\n",
       "      <td>boro suffer morrison injury blow middlesbrough...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ArticleId                                               Text\n",
       "0       1018  qpr keeper day heads for preston queens park r...\n",
       "1       1319  software watching while you work software that...\n",
       "2       1138  d arcy injury adds to ireland woe gordon d arc...\n",
       "3        459  india s reliance family feud heats up the ongo...\n",
       "4       1020  boro suffer morrison injury blow middlesbrough..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc12446d",
   "metadata": {},
   "source": [
    "Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97e1c3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e5a5b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/home/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /Users/home/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4ca1cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46cdbb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = str.maketrans('', '', string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be1ba198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessDataset(train_text):\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  words = word_tokenize(train_text)\n",
    "  words = [word for word in words if not word in stop_words]\n",
    "  stemmer= PorterStemmer()\n",
    "  stem_text=' '.join([stemmer.stem(word.translate(translator)) for word in words])\n",
    "  return stem_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79095c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'worldcom exboss launch defenc lawyer defend former worldcom chief berni ebber batteri fraud charg call compani whistleblow first wit  cynthia cooper worldcom exhead intern account alert director irregular account practic us telecom giant 2002 warn led collaps firm follow discoveri  11bn  £57bn  account fraud  mr ebber plead guilti charg fraud conspiraci  prosecut lawyer argu mr ebber orchestr seri account trick worldcom order employe hide expens inflat revenu meet wall street earn estim  ms cooper run consult busi told juri new york wednesday extern auditor arthur andersen approv worldcom account earli 2001 2002 said andersen given green light procedur practic use worldcom  mr ebber lawyer said unawar fraud argu auditor alert problem  ms cooper also said sharehold meet mr ebber often pass technic question compani financ chief give brief answer  prosecut star wit former worldcom financi chief scott sullivan said mr ebber order account adjust firm tell hit book  howev ms cooper said mr sullivan mention anyth uncomfort worldcom account 2001 audit committe meet  mr ebber could face jail sentenc 85 year convict charg face  worldcom emerg bankruptci protect 2004 known mci  last week mci agre buyout verizon commun deal valu  675bn '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessDataset(df_train.iloc[0].Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a009d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'worldcom ex-boss launches defence lawyers defending former worldcom chief bernie ebbers against a battery of fraud charges have called a company whistleblower as their first witness.  cynthia cooper  worldcom s ex-head of internal accounting  alerted directors to irregular accounting practices at the us telecoms giant in 2002. her warnings led to the collapse of the firm following the discovery of an $11bn (£5.7bn) accounting fraud. mr ebbers has pleaded not guilty to charges of fraud and conspiracy.  prosecution lawyers have argued that mr ebbers orchestrated a series of accounting tricks at worldcom  ordering employees to hide expenses and inflate revenues to meet wall street earnings estimates. but ms cooper  who now runs her own consulting business  told a jury in new york on wednesday that external auditors arthur andersen had approved worldcom s accounting in early 2001 and 2002. she said andersen had given a  green light  to the procedures and practices used by worldcom. mr ebber s lawyers have said he was unaware of the fraud  arguing that auditors did not alert him to any problems.  ms cooper also said that during shareholder meetings mr ebbers often passed over technical questions to the company s finance chief  giving only  brief  answers himself. the prosecution s star witness  former worldcom financial chief scott sullivan  has said that mr ebbers ordered accounting adjustments at the firm  telling him to  hit our books . however  ms cooper said mr sullivan had not mentioned  anything uncomfortable  about worldcom s accounting during a 2001 audit committee meeting. mr ebbers could face a jail sentence of 85 years if convicted of all the charges he is facing. worldcom emerged from bankruptcy protection in 2004  and is now known as mci. last week  mci agreed to a buyout by verizon communications in a deal valued at $6.75bn.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[0].Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b689138",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['TextCleaned'] = df_train['Text'].apply(preprocessDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de641372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       worldcom exboss launch defenc lawyer defend fo...\n",
       "1       german busi confid slide german busi confid fe...\n",
       "2       bbc poll indic econom gloom citizen major nati...\n",
       "3       lifestyl govern mobil choic faster better funk...\n",
       "4       enron boss  168m payout eighteen former enron ...\n",
       "                              ...                        \n",
       "1485    doubl evict big brother model capric holbi cit...\n",
       "1486    dj doubl act revamp chart show dj duo jk joel ...\n",
       "1487    weak dollar hit reuter revenu media group reut...\n",
       "1488    appl ipod famili expand market appl expand ipo...\n",
       "1489    santi worm make unwelcom visit thousand websit...\n",
       "Name: TextCleaned, Length: 1490, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['TextCleaned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3602aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc154f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1490x19645 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 218394 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = TfidfVectorizer()\n",
    "X = vect.fit_transform(df_train.TextCleaned)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d02d465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.02407628, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.02546556, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f165d93",
   "metadata": {},
   "source": [
    "Now let's build and train our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44db5bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56d94ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NMF(n_components=5, random_state=42)\n",
    "W = model.fit_transform(X)\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f30a689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1490, 5)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1671e689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 19645)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98e32230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.92692014e-04, 4.07263255e-02, 1.24470770e-02, 5.30154711e-03,\n",
       "        5.88158134e-02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        2.14560228e-01],\n",
       "       [1.47048247e-02, 3.34845561e-02, 2.23136449e-02, 0.00000000e+00,\n",
       "        1.21383956e-01],\n",
       "       ...,\n",
       "       [8.27934717e-03, 0.00000000e+00, 1.19923608e-04, 8.25323565e-03,\n",
       "        1.60985809e-01],\n",
       "       [0.00000000e+00, 0.00000000e+00, 2.18646066e-01, 1.38263954e-02,\n",
       "        2.41925381e-02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.15825008e-01, 0.00000000e+00,\n",
       "        0.00000000e+00]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d857d6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label = np.squeeze(np.asarray(W.argmax(axis=1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d60f67fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 4, ..., 4, 2, 2])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "527f688e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1490,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_label.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "facbab3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(predicted_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f5a3fb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_categ = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e41f879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "  label_to_categ[i] = df_train.iloc[np.where(predicted_label == i)[0]]['Category'].value_counts().idxmax()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2b1ed851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'sport', 1: 'politics', 2: 'tech', 3: 'entertainment', 4: 'business'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_categ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a4179f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fe9c5421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['business', 'business', 'business', ..., 'business', 'tech',\n",
       "       'tech'], dtype='<U13')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_categ = np.vectorize(label_to_categ.get)(predicted_label)\n",
    "predicted_categ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "564a8128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9194630872483222"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(df_train.Category, predicted_categ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d04fcd72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[312,   1,  13,   0,  10],\n",
       "       [ 10, 226,   6,   3,  28],\n",
       "       [ 18,   0, 249,   3,   4],\n",
       "       [  2,   4,   0, 340,   0],\n",
       "       [  4,   6,   3,   5, 243]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(df_train.Category, predicted_categ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8076a8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_categ = np.vectorize(label_to_categ.get)(model.transform(vect.transform(df_test['Text'].apply(preprocessDataset))).argmax(axis=1))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "087936fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_pred_categ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5c263977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(735,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_categ.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fc9c31ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Category'] = test_pred_categ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "71dcc9c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1018</td>\n",
       "      <td>qpr keeper day heads for preston queens park r...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1319</td>\n",
       "      <td>software watching while you work software that...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1138</td>\n",
       "      <td>d arcy injury adds to ireland woe gordon d arc...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>459</td>\n",
       "      <td>india s reliance family feud heats up the ongo...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1020</td>\n",
       "      <td>boro suffer morrison injury blow middlesbrough...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>1923</td>\n",
       "      <td>eu to probe alitalia  state aid  the european ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>373</td>\n",
       "      <td>u2 to play at grammy awards show irish rock ba...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>1704</td>\n",
       "      <td>sport betting rules in spotlight a group of mp...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>206</td>\n",
       "      <td>alfa romeos  to get gm engines  fiat is to sto...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>471</td>\n",
       "      <td>citizenship event for 18s touted citizenship c...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>735 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ArticleId                                               Text  \\\n",
       "0         1018  qpr keeper day heads for preston queens park r...   \n",
       "1         1319  software watching while you work software that...   \n",
       "2         1138  d arcy injury adds to ireland woe gordon d arc...   \n",
       "3          459  india s reliance family feud heats up the ongo...   \n",
       "4         1020  boro suffer morrison injury blow middlesbrough...   \n",
       "..         ...                                                ...   \n",
       "730       1923  eu to probe alitalia  state aid  the european ...   \n",
       "731        373  u2 to play at grammy awards show irish rock ba...   \n",
       "732       1704  sport betting rules in spotlight a group of mp...   \n",
       "733        206  alfa romeos  to get gm engines  fiat is to sto...   \n",
       "734        471  citizenship event for 18s touted citizenship c...   \n",
       "\n",
       "          Category  \n",
       "0            sport  \n",
       "1             tech  \n",
       "2            sport  \n",
       "3         business  \n",
       "4            sport  \n",
       "..             ...  \n",
       "730       business  \n",
       "731  entertainment  \n",
       "732           tech  \n",
       "733       business  \n",
       "734       politics  \n",
       "\n",
       "[735 rows x 3 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a1ccb315",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.drop(['Text'], axis = 1).to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f680ebcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'sport', 1: 'politics', 2: 'tech', 3: 'entertainment', 4: 'business'}\n"
     ]
    }
   ],
   "source": [
    "model_kl = NMF(n_components=5, random_state=42, solver = 'mu', beta_loss = 'kullback-leibler')\n",
    "W_kl = model_kl.fit_transform(X)\n",
    "predicted_label_kl = np.squeeze(np.asarray(W_kl.argmax(axis=1)))\n",
    "label_to_categ_kl = {}\n",
    "for i in range(5):\n",
    "  label_to_categ_kl[i] = df_train.iloc[np.where(predicted_label_kl == i)[0]]['Category'].value_counts().idxmax()\n",
    "print(label_to_categ_kl)\n",
    "test_pred_categ_kl = np.vectorize(label_to_categ_kl.get)(model_kl.transform(vect.transform(df_test['Text'].apply(preprocessDataset))).argmax(axis=1))\n",
    "df_test['Category'] = test_pred_categ_kl\n",
    "df_test.drop(['Text'], axis = 1).to_csv('submission_kl.csv', index = False)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c0df6c65",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "np.matrix is not supported. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_is \u001b[38;5;241m=\u001b[39m NMF(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, solver \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmu\u001b[39m\u001b[38;5;124m'\u001b[39m, beta_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitakura-saito\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m W_is \u001b[38;5;241m=\u001b[39m model_is\u001b[38;5;241m.\u001b[39mfit_transform(X\u001b[38;5;241m.\u001b[39mtodense()\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m0.00000001\u001b[39m)\n\u001b[1;32m      3\u001b[0m predicted_label_is \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(np\u001b[38;5;241m.\u001b[39masarray(W_is\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)))\n\u001b[1;32m      4\u001b[0m label_to_categ_is \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/decomposition/_nmf.py:1601\u001b[0m, in \u001b[0;36mNMF.fit_transform\u001b[0;34m(self, X, y, W, H)\u001b[0m\n\u001b[1;32m   1573\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1574\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, W\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, H\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1575\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Learn a NMF model for the data X and returns the transformed data.\u001b[39;00m\n\u001b[1;32m   1576\u001b[0m \n\u001b[1;32m   1577\u001b[0m \u001b[38;5;124;03m    This is more efficient than calling fit followed by transform.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1599\u001b[0m \u001b[38;5;124;03m        Transformed data.\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1601\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m   1602\u001b[0m         X, accept_sparse\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m), dtype\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[1;32m   1603\u001b[0m     )\n\u001b[1;32m   1605\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(assume_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1606\u001b[0m         W, H, n_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_transform(X, W\u001b[38;5;241m=\u001b[39mW, H\u001b[38;5;241m=\u001b[39mH)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    602\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 604\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    606\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:753\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Input validation on an array, list, sparse matrix or similar.\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \n\u001b[1;32m    664\u001b[0m \u001b[38;5;124;03mBy default, the input is checked to be a non-empty 2D array containing\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;124;03m    The converted and validated array.\u001b[39;00m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(array, np\u001b[38;5;241m.\u001b[39mmatrix):\n\u001b[0;32m--> 753\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    754\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.matrix is not supported. Please convert to a numpy array with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    755\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.asarray. For more information see: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    756\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://numpy.org/doc/stable/reference/generated/numpy.matrix.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    757\u001b[0m     )\n\u001b[1;32m    759\u001b[0m xp, is_array_api_compliant \u001b[38;5;241m=\u001b[39m get_namespace(array)\n\u001b[1;32m    761\u001b[0m \u001b[38;5;66;03m# store reference to original array to check if copy is needed when\u001b[39;00m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;66;03m# function returns\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: np.matrix is not supported. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html"
     ]
    }
   ],
   "source": [
    "model_is = NMF(n_components=5, random_state=42, solver = 'mu', beta_loss = 'itakura-saito')\n",
    "W_is = model_is.fit_transform(X.todense()+0.00000001)\n",
    "predicted_label_is = np.squeeze(np.asarray(W_is.argmax(axis=1)))\n",
    "label_to_categ_is = {}\n",
    "for i in range(5):\n",
    "  label_to_categ_is[i] = df_train.iloc[np.where(predicted_label_is == i)[0]]['Category'].value_counts().idxmax()\n",
    "print(label_to_categ_is)\n",
    "test_pred_categ_is = np.vectorize(label_to_categ_is.get)(model_is.transform(vect.transform(df_test['Text'].apply(preprocessDataset)).todense()+0.00000001).argmax(axis=1))\n",
    "df_test['Category'] = test_pred_categ_is\n",
    "df_test.drop(['Text'], axis = 1).to_csv('submission_is.csv', index = False)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ba2603ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'sport', 1: 'politics', 2: 'business', 3: 'entertainment', 4: 'tech'}\n",
      "{0: 'business', 1: 'sport', 2: 'business', 3: 'entertainment', 4: 'tech'}\n",
      "{0: 'business', 1: 'sport', 2: 'entertainment', 3: 'politics', 4: 'tech'}\n"
     ]
    }
   ],
   "source": [
    "for min_df in [10,50,100]:\n",
    "  vect = TfidfVectorizer(min_df = min_df)\n",
    "  X = vect.fit_transform(df_train.TextCleaned)\n",
    "  model_kl = NMF(n_components=5, random_state=42, solver = 'mu', beta_loss = 'kullback-leibler')\n",
    "  W_kl = model_kl.fit_transform(X)\n",
    "  predicted_label_kl = np.squeeze(np.asarray(W_kl.argmax(axis=1)))\n",
    "  label_to_categ_kl = {}\n",
    "  for i in range(5):\n",
    "    label_to_categ_kl[i] = df_train.iloc[np.where(predicted_label_kl == i)[0]]['Category'].value_counts().idxmax()\n",
    "  print(label_to_categ_kl)\n",
    "  test_pred_categ_kl = np.vectorize(label_to_categ_kl.get)(model_kl.transform(vect.transform(df_test['Text'].apply(preprocessDataset))).argmax(axis=1))\n",
    "  df_test['Category'] = test_pred_categ_kl\n",
    "  df_test.drop(['Text'], axis = 1).to_csv('min_df_'+str(min_df)+'_submission_kl.csv', index = False)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f47ec87",
   "metadata": {},
   "source": [
    "Let's compare with Supervised Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b8631ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f0321f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6ee8477d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.fit(X, df_train.Category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "343f1af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.score(X, df_train.Category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5a4f18f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Category'] = rfc.predict(vect.transform(df_test['Text'].apply(preprocessDataset)))\n",
    "df_test.drop(['Text'], axis = 1).to_csv('submission_rfc.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "713912fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'tech', 1: 'politics', 2: 'entertainment', 3: 'business', 4: 'sport'}\n",
      "{0: 'sport', 1: 'politics', 2: 'business', 3: 'entertainment', 4: 'tech'}\n",
      "{0: 'tech', 1: 'politics', 2: 'sport', 3: 'entertainment', 4: 'business'}\n"
     ]
    }
   ],
   "source": [
    "for frac in [0.1,0.2,0.5]:\n",
    "  df_train_sample = df_train.sample(frac=frac, random_state=42)\n",
    "  df_train_sample['TextCleaned'] = df_train_sample['Text'].apply(preprocessDataset)\n",
    "  #nmf kl\n",
    "  vect = TfidfVectorizer()\n",
    "  X = vect.fit_transform(df_train_sample.TextCleaned)\n",
    "  model_kl = NMF(n_components=5, random_state=42, solver = 'mu', beta_loss = 'kullback-leibler')\n",
    "  W_kl = model_kl.fit_transform(X)\n",
    "  predicted_label_kl = np.squeeze(np.asarray(W_kl.argmax(axis=1)))\n",
    "  label_to_categ_kl = {}\n",
    "  for i in range(5):\n",
    "    label_to_categ_kl[i] = df_train_sample.iloc[np.where(predicted_label_kl == i)[0]]['Category'].value_counts().idxmax()\n",
    "  print(label_to_categ_kl)\n",
    "  test_pred_categ_kl = np.vectorize(label_to_categ_kl.get)(model_kl.transform(vect.transform(df_test['Text'].apply(preprocessDataset))).argmax(axis=1))\n",
    "  df_test['Category'] = test_pred_categ_kl\n",
    "  df_test.drop(['Text'], axis = 1).to_csv('submission_kl_frac'+str(frac)+'.csv', index = False)\n",
    "  #rfc\n",
    "  rfc = RandomForestClassifier(random_state=42)\n",
    "  rfc.fit(X, df_train_sample.Category)\n",
    "  df_test['Category'] = rfc.predict(vect.transform(df_test['Text'].apply(preprocessDataset)))\n",
    "  df_test.drop(['Text'], axis = 1).to_csv('submission_rfc_frac'+str(frac)+'.csv', index = False)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b09804",
   "metadata": {},
   "source": [
    "Here is a comparison of NMF (kl) Score vs Random Forest Classifier Score on train data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae444fe6",
   "metadata": {},
   "source": [
    "100% train data\n",
    "    NMF Score: 0.940\n",
    "    RandomForestClassifier Score:0.946\n",
    "    \n",
    "50% train data\n",
    "    NMF Score: 0.916\n",
    "    RandomForestClassifier Score:0.951\n",
    "    \n",
    "20% train data\n",
    "    NMF Score:0.932\n",
    "    RandomForestClassifier Score:0.936\n",
    "    \n",
    "10% train data\n",
    "    NMF Score:0.829\n",
    "    RandomForestClassifier Score:0.849"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bb37c4",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "- Random Forest Classifier performs best with 50% training data\n",
    "- NMF performs best with 100% training data\n",
    "- Random Forest Classifier has an overfitting problem at 100% whereas NMF works better comparing the improvement from 10 to 20% training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1225a45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
